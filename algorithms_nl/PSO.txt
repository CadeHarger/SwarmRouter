Standard (gbest) PSO
Description: Particles share a single global best position g, updating velocities with inertia w, cognitive coefficient Ï†â‚š, and social coefficient Ï†g. 
Wikipedia

Pseudocode:

For each particle i = 1â€¦S:
  xáµ¢ â† U(b_lo, b_up)
  váµ¢ â† U(âˆ’|b_upâˆ’b_lo|, |b_upâˆ’b_lo|)
  páµ¢ â† xáµ¢
  if f(páµ¢) < f(g) then g â† páµ¢
While not termination:
  For each i:
    For each dimension d:
      râ‚š,rg â† U(0,1)
      váµ¢,d â† wÂ·váµ¢,d + Ï†â‚šÂ·râ‚šÂ·(páµ¢,dâˆ’xáµ¢,d) + Ï†gÂ·rgÂ·(g_dâˆ’xáµ¢,d)
    xáµ¢ â† xáµ¢ + váµ¢
    if f(xáµ¢) < f(páµ¢): páµ¢ â† xáµ¢; if f(páµ¢) < f(g): g â† páµ¢
``` :contentReference[oaicite:1]{index=1}

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Topology Variants
Global Best (gbest)
All particles observe the same g (above). 

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Local Best (lbest) PSO
Description: Each particle i updates based on its neighborhoodâ€™s best láµ¢ rather than global best, promoting exploration. Common topologies include ring (each particle has two neighbors). 
Wikipedia
GitHub

Pseudocode Differences: Replace g with láµ¢ (the best personal best among neighbors) in velocity update. 

https://github.com/nadaabdelrahman16/local-best-PSO/blob/main/local-best-PSO.py?utm_source=chatgpt.com
# ================================================================================================================================

Parameterâ€‘Controlled Variants
Inertia Weight PSO
Concept: Dynamically adjust inertia w to balance exploration (high w) and exploitation (low w). 
Wikipedia

Update: As in Standard PSO, but w may linearly decrease from wâ‚˜â‚â‚“ to wâ‚˜áµ¢â‚™ over iterations. 
Wikipedia

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Constriction Factor PSO
Description: Introduces constriction factor Ï‡ to guarantee convergence by scaling velocity. 
GMSARN Journal

Velocity Update:
ğ‘£
ğ‘–
,
ğ‘‘
â†
ğœ’
[
ğ‘£
ğ‘–
,
ğ‘‘
+
ğœ‘
ğ‘
â‹…
ğ‘Ÿ
ğ‘
â‹…
(
ğ‘
ğ‘–
,
ğ‘‘
âˆ’
ğ‘¥
ğ‘–
,
ğ‘‘
)
+
ğœ‘
ğ‘”
â‹…
ğ‘Ÿ
ğ‘”
â‹…
(
ğ‘”
ğ‘‘
âˆ’
ğ‘¥
ğ‘–
,
ğ‘‘
)
]
v 
i,d
â€‹
 â†Ï‡[v 
i,d
â€‹
 +Ï† 
p
â€‹
 â‹…r 
p
â€‹
 â‹…(p 
i,d
â€‹
 âˆ’x 
i,d
â€‹
 )+Ï†gâ‹…rgâ‹…(g 
d
â€‹
 âˆ’x 
i,d
â€‹
 )] 
GMSARN Journal

https://gmsarnjournal.com/home/wp-content/uploads/2015/08/vol7no1-4.pdf?utm_source=chatgpt.com
# ================================================================================================================================

Adaptive PSO (APSO)
Description: Automatically tunes w, Ï†â‚š, and Ï†g based on swarm state (exploration, exploitation, convergence, jumping out) during runtime. 
GitHub

Core Steps:

Estimate evolutionary state from population distribution and fitness.

Adjust parameters (inertia, coefficients) accordingly.

Perform standard PSO updates. 
GitHub

https://github.com/SakuraAyase/Adaptive-PSO?utm_source=chatgpt.com
# ================================================================================================================================

Simplified PSO
Bareâ€‘Bones PSO
Description: Eliminates velocity; positions sampled from Gaussian around personal and global bests. 
Wikipedia

Update Rule:
ğ‘¥
ğ‘–
âˆ¼
ğ‘
(
ğ‘
ğ‘–
+
ğ‘”
2
,
â€‰
âˆ£
ğ‘
ğ‘–
âˆ’
ğ‘”
âˆ£
)
x 
i
â€‹
 âˆ¼N( 
2
p 
i
â€‹
 +g
â€‹
 ,âˆ£p 
i
â€‹
 âˆ’gâˆ£) 
Wikipedia

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Accelerated PSO
Description: Dispenses with velocity and personal best. Moves directly toward global best with random perturbation. 
Wikipedia

Update Rule:

ğ‘¥
ğ‘–
â†
(
1
âˆ’
ğ›½
)
ğ‘¥
ğ‘–
+
ğ›½
ğ‘”
+
ğ›¼
ğ¿
â‹…
ğ‘¢
,
ğ‘¢
âˆ¼
ğ‘ˆ
(
âˆ’
1
,
1
)
x 
i
â€‹
 â†(1âˆ’Î²)x 
i
â€‹
 +Î²g+Î±Lâ‹…u,uâˆ¼U(âˆ’1,1)
where Î±,Î² control step size; L is problem scale. 
Wikipedia

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Discrete and Binary PSO
Binary PSO (BPSO)
Description: For combinatorial domains, velocity continuous but position binary via sigmoid transformation. 
Scholarpedia

For each i,d:
  váµ¢,d update as in Standard PSO
  S(váµ¢,d) = 1 / (1 + exp(âˆ’váµ¢,d))
  xáµ¢,d = 1 if U(0,1) < S(váµ¢,d) else 0
``` :contentReference[oaicite:16]{index=16}  

https://www.scholarpedia.org/article/Particle_swarm_optimization?utm_source=chatgpt.com
# ================================================================================================================================

Learningâ€‘Based PSO
Comprehensive Learning PSO (CLPSO)
Description: Each particle learns from different exemplars selected from neighbors, enhancing diversity. 
GitHub

Core Idea: For each dimension, pick exemplar from random particleâ€™s personal best based on probability; update velocity accordingly. 

https://github.com/thuchula6792/CLPSO?utm_source=chatgpt.com
# ================================================================================================================================

Fully Informed PSO (FIPS)
Description: Particles consider all k-nearest neighborsâ€™ personal bests, not just the best, in social component. 
matheubotha.github.io

Velocity Update:

ğ‘£
ğ‘–
â†
ğ‘¤
ğ‘£
ğ‘–
+
1
âˆ£
ğ‘
ğ‘–
âˆ£
âˆ‘
ğ‘—
âˆˆ
ğ‘
ğ‘–
ğœ‘
â‹…
ğ‘Ÿ
â‹…
(
ğ‘
ğ‘—
âˆ’
ğ‘¥
ğ‘–
)
v 
i
â€‹
 â†wv 
i
â€‹
 + 
âˆ£N 
i
â€‹
 âˆ£
1
â€‹
  
jâˆˆN 
i
â€‹
 
âˆ‘
â€‹
 Ï†â‹…râ‹…(p 
j
â€‹
 âˆ’x 
i
â€‹
 )
where Náµ¢ is neighborhood set. 
matheubotha.github.io

https://matheubotha.github.io/DragonBrain/optimizers/fips_pso.html?utm_source=chatgpt.com
# ================================================================================================================================

Quantumâ€‘Behaved PSO
QPSO
Description: Uses quantum mechanics principles; particles have probability density functions instead of explicit velocity. 
GitHub

Position Update:

ğ‘¥
ğ‘–
ğ‘¡
+
1
=
ğ‘ƒ
ğ‘–
ğ‘¡
Â±
ğ›½
â‹…
âˆ£
ğ‘€
ğ‘¡
âˆ’
ğ‘¥
ğ‘–
ğ‘¡
âˆ£
â‹…
ln
â¡
â€‰â£
(
1
ğ‘¢
)
x 
i
t+1
â€‹
 =P 
i
t
â€‹
 Â±Î²â‹…âˆ£M 
t
 âˆ’x 
i
t
â€‹
 âˆ£â‹…ln( 
u
1
â€‹
 )
where Páµ¢ is attractor, M is mean best, uâˆ¼U(0,1), Î² is contractionâ€“expansion coefficient. 
GitHub

https://github.com/ngroup/qpso?utm_source=chatgpt.com
# ================================================================================================================================

Multiâ€‘Objective PSO
MOPSO
Description: Extends PSO with Pareto dominance, archives nonâ€‘dominated solutions, uses leaders selected from archives. 
MathWorks - Maker of MATLAB and Simulink

Implementation: MATLAB file MPSO.m demonstrates archive maintenance and velocity update with randomly chosen leaders. 
MathWorks - Maker of MATLAB and Simulink

https://www.mathworks.com/matlabcentral/fileexchange/62074-multi-objective-particle-swarm-optimization-mopso?utm_source=chatgpt.com
# ================================================================================================================================

Hybrid PSO Variants
PSOâ€‘DE Hybrid
PSOâ€‘DE (Parallel): Executes PSO and DE operators in parallel, exchanging information to mitigate stagnation. 
ScienceDirect
SpringerLink

https://www.sciencedirect.com/science/article/pii/S1568494609001550?utm_source=chatgpt.com
# ================================================================================================================================


PSOâ€‘DE (Sequential): Applies DE mutation on personal bests each iteration, then PSO updates. 
MDPI


https://www.mdpi.com/2227-7390/11/3/522?utm_source=chatgpt.com
# ================================================================================================================================



