Standard (gbest) PSO
Description: Particles share a single global best position g, updating velocities with inertia w, cognitive coefficient φₚ, and social coefficient φg. 
Wikipedia

Pseudocode:

For each particle i = 1…S:
  xᵢ ← U(b_lo, b_up)
  vᵢ ← U(−|b_up−b_lo|, |b_up−b_lo|)
  pᵢ ← xᵢ
  if f(pᵢ) < f(g) then g ← pᵢ
While not termination:
  For each i:
    For each dimension d:
      rₚ,rg ← U(0,1)
      vᵢ,d ← w·vᵢ,d + φₚ·rₚ·(pᵢ,d−xᵢ,d) + φg·rg·(g_d−xᵢ,d)
    xᵢ ← xᵢ + vᵢ
    if f(xᵢ) < f(pᵢ): pᵢ ← xᵢ; if f(pᵢ) < f(g): g ← pᵢ
``` :contentReference[oaicite:1]{index=1}

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Topology Variants
Global Best (gbest)
All particles observe the same g (above). 

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Local Best (lbest) PSO
Description: Each particle i updates based on its neighborhood’s best lᵢ rather than global best, promoting exploration. Common topologies include ring (each particle has two neighbors). 
Wikipedia
GitHub

Pseudocode Differences: Replace g with lᵢ (the best personal best among neighbors) in velocity update. 

https://github.com/nadaabdelrahman16/local-best-PSO/blob/main/local-best-PSO.py?utm_source=chatgpt.com
# ================================================================================================================================

Parameter‑Controlled Variants
Inertia Weight PSO
Concept: Dynamically adjust inertia w to balance exploration (high w) and exploitation (low w). 
Wikipedia

Update: As in Standard PSO, but w may linearly decrease from wₘₐₓ to wₘᵢₙ over iterations. 
Wikipedia

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Constriction Factor PSO
Description: Introduces constriction factor χ to guarantee convergence by scaling velocity. 
GMSARN Journal

Velocity Update:
𝑣
𝑖
,
𝑑
←
𝜒
[
𝑣
𝑖
,
𝑑
+
𝜑
𝑝
⋅
𝑟
𝑝
⋅
(
𝑝
𝑖
,
𝑑
−
𝑥
𝑖
,
𝑑
)
+
𝜑
𝑔
⋅
𝑟
𝑔
⋅
(
𝑔
𝑑
−
𝑥
𝑖
,
𝑑
)
]
v 
i,d
​
 ←χ[v 
i,d
​
 +φ 
p
​
 ⋅r 
p
​
 ⋅(p 
i,d
​
 −x 
i,d
​
 )+φg⋅rg⋅(g 
d
​
 −x 
i,d
​
 )] 
GMSARN Journal

https://gmsarnjournal.com/home/wp-content/uploads/2015/08/vol7no1-4.pdf?utm_source=chatgpt.com
# ================================================================================================================================

Adaptive PSO (APSO)
Description: Automatically tunes w, φₚ, and φg based on swarm state (exploration, exploitation, convergence, jumping out) during runtime. 
GitHub

Core Steps:

Estimate evolutionary state from population distribution and fitness.

Adjust parameters (inertia, coefficients) accordingly.

Perform standard PSO updates. 
GitHub

https://github.com/SakuraAyase/Adaptive-PSO?utm_source=chatgpt.com
# ================================================================================================================================

Simplified PSO
Bare‑Bones PSO
Description: Eliminates velocity; positions sampled from Gaussian around personal and global bests. 
Wikipedia

Update Rule:
𝑥
𝑖
∼
𝑁
(
𝑝
𝑖
+
𝑔
2
,
 
∣
𝑝
𝑖
−
𝑔
∣
)
x 
i
​
 ∼N( 
2
p 
i
​
 +g
​
 ,∣p 
i
​
 −g∣) 
Wikipedia

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Accelerated PSO
Description: Dispenses with velocity and personal best. Moves directly toward global best with random perturbation. 
Wikipedia

Update Rule:

𝑥
𝑖
←
(
1
−
𝛽
)
𝑥
𝑖
+
𝛽
𝑔
+
𝛼
𝐿
⋅
𝑢
,
𝑢
∼
𝑈
(
−
1
,
1
)
x 
i
​
 ←(1−β)x 
i
​
 +βg+αL⋅u,u∼U(−1,1)
where α,β control step size; L is problem scale. 
Wikipedia

https://en.wikipedia.org/wiki/Particle_swarm_optimization
# ================================================================================================================================

Discrete and Binary PSO
Binary PSO (BPSO)
Description: For combinatorial domains, velocity continuous but position binary via sigmoid transformation. 
Scholarpedia

For each i,d:
  vᵢ,d update as in Standard PSO
  S(vᵢ,d) = 1 / (1 + exp(−vᵢ,d))
  xᵢ,d = 1 if U(0,1) < S(vᵢ,d) else 0
``` :contentReference[oaicite:16]{index=16}  

https://www.scholarpedia.org/article/Particle_swarm_optimization?utm_source=chatgpt.com
# ================================================================================================================================

Learning‑Based PSO
Comprehensive Learning PSO (CLPSO)
Description: Each particle learns from different exemplars selected from neighbors, enhancing diversity. 
GitHub

Core Idea: For each dimension, pick exemplar from random particle’s personal best based on probability; update velocity accordingly. 

https://github.com/thuchula6792/CLPSO?utm_source=chatgpt.com
# ================================================================================================================================

Fully Informed PSO (FIPS)
Description: Particles consider all k-nearest neighbors’ personal bests, not just the best, in social component. 
matheubotha.github.io

Velocity Update:

𝑣
𝑖
←
𝑤
𝑣
𝑖
+
1
∣
𝑁
𝑖
∣
∑
𝑗
∈
𝑁
𝑖
𝜑
⋅
𝑟
⋅
(
𝑝
𝑗
−
𝑥
𝑖
)
v 
i
​
 ←wv 
i
​
 + 
∣N 
i
​
 ∣
1
​
  
j∈N 
i
​
 
∑
​
 φ⋅r⋅(p 
j
​
 −x 
i
​
 )
where Nᵢ is neighborhood set. 
matheubotha.github.io

https://matheubotha.github.io/DragonBrain/optimizers/fips_pso.html?utm_source=chatgpt.com
# ================================================================================================================================

Quantum‑Behaved PSO
QPSO
Description: Uses quantum mechanics principles; particles have probability density functions instead of explicit velocity. 
GitHub

Position Update:

𝑥
𝑖
𝑡
+
1
=
𝑃
𝑖
𝑡
±
𝛽
⋅
∣
𝑀
𝑡
−
𝑥
𝑖
𝑡
∣
⋅
ln
⁡
 ⁣
(
1
𝑢
)
x 
i
t+1
​
 =P 
i
t
​
 ±β⋅∣M 
t
 −x 
i
t
​
 ∣⋅ln( 
u
1
​
 )
where Pᵢ is attractor, M is mean best, u∼U(0,1), β is contraction–expansion coefficient. 
GitHub

https://github.com/ngroup/qpso?utm_source=chatgpt.com
# ================================================================================================================================

Multi‑Objective PSO
MOPSO
Description: Extends PSO with Pareto dominance, archives non‑dominated solutions, uses leaders selected from archives. 
MathWorks - Maker of MATLAB and Simulink

Implementation: MATLAB file MPSO.m demonstrates archive maintenance and velocity update with randomly chosen leaders. 
MathWorks - Maker of MATLAB and Simulink

https://www.mathworks.com/matlabcentral/fileexchange/62074-multi-objective-particle-swarm-optimization-mopso?utm_source=chatgpt.com
# ================================================================================================================================

Hybrid PSO Variants
PSO‑DE Hybrid
PSO‑DE (Parallel): Executes PSO and DE operators in parallel, exchanging information to mitigate stagnation. 
ScienceDirect
SpringerLink

https://www.sciencedirect.com/science/article/pii/S1568494609001550?utm_source=chatgpt.com
# ================================================================================================================================


PSO‑DE (Sequential): Applies DE mutation on personal bests each iteration, then PSO updates. 
MDPI


https://www.mdpi.com/2227-7390/11/3/522?utm_source=chatgpt.com
# ================================================================================================================================



